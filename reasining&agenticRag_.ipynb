{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/Reasoning-RAG-with-deep-seek/blob/main/reasining%26agenticRag_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUaCO1SWHE7j",
        "outputId": "999ca27b-dd03-4239-8652-c0b4058f0d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.5/232.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 datasets-3.5.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multiprocess-0.70.16 propcache-0.3.1 xxhash-3.5.0 yarl-1.18.3\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw9oW4qzIIoS",
        "outputId": "e9275953-1d8d-44ba-e08c-6739ba485147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
            "  Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.3.19-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
            "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
            "Downloading langsmith-0.3.19-py3-none-any.whl (351 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.9/351.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: zstandard, mypy-extensions, marshmallow, jsonpointer, httpx-sse, greenlet, typing-inspect, SQLAlchemy, requests-toolbelt, jsonpatch, pydantic-settings, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "Successfully installed SQLAlchemy-2.0.40 dataclasses-json-0.6.7 greenlet-3.1.1 httpx-sse-0.4.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.21 langchain-core-0.3.49 langchain-text-splitters-0.3.7 langchain_community-0.3.20 langsmith-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 requests-toolbelt-1.0.0 typing-inspect-0.9.0 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt7vSyt59Kd2",
        "outputId": "0e250bb4-66f8-4721-ce80-0a4256ee7b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cryptography, pdfminer.six\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 3.4.8\n",
            "    Uninstalling cryptography-3.4.8:\n",
            "      Successfully uninstalled cryptography-3.4.8\n",
            "Successfully installed cryptography-44.0.2 pdfminer.six-20250327\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV9HBpEy9Q5w",
        "outputId": "26389a12-92bc-40ff-bc8a-e786816bc530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNfcJNpt-4Ze",
        "outputId": "57936424-0346-4779-e002-9f96a2b65b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d815108048d842bf8f7c78134f388d8a",
            "249ae3e360a74498994d7ffe93199893",
            "d95ec3aeb7bf4e5a8dcf3eb968d35fef",
            "f5162a6267484bca9e633a3c349484e8",
            "322d0f8e6e564cb388b60ee45bcf03db",
            "9f6d4fe9dda84c7fa1c29c4e40867b81",
            "2a721b251eea48198822c49b07380dd5",
            "5e9d99dccd554c6c8419d6b6e3acdd63",
            "4985ca2324874245b42d1e26a313d052",
            "e193fef3bcdc435896d9b7287e4e6ab1",
            "5c51202cfe1b44f597ab5d4452c916f6"
          ]
        },
        "id": "Yeg0qHdYHnht",
        "outputId": "989b17b3-9add-4643-adb8-7da9f5c8ddba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d815108048d842bf8f7c78134f388d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter the directory path containing your documents: /content/Document\n",
            "Loading documents from /content/Document...\n",
            "Loaded 1 documents.\n",
            "Processing documents...\n",
            "Created 4 chunks.\n",
            "Creating vector store...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created successfully.\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: Morocco is known to be inhabited ?\n",
            "Query: Morocco is known to be inhabited ?\n",
            "Error processing query: 'FAISS' object has no attribute 'similarity_search_by_vector_with_relevance_scores'\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): what is the population of Morocco ?\n",
            "Invalid choice. Please try again.\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter the directory path containing your documents: what is the population of Morocco ?\n",
            "Loading documents from what is the population of Morocco ?...\n",
            "Error loading *.txt documents: Directory not found: 'what is the population of Morocco ?'\n",
            "Error loading *.pdf documents: Directory not found: 'what is the population of Morocco ?'\n",
            "Error loading *.md documents: Directory not found: 'what is the population of Morocco ?'\n",
            "Error loading *.csv documents: Directory not found: 'what is the population of Morocco ?'\n",
            "Loaded 0 documents.\n",
            "Processing documents...\n",
            "Created 0 chunks.\n",
            "Creating vector store...\n",
            "Error ingesting documents: not enough values to unpack (expected 2, got 0)\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: what is the population of Morocco ?\n",
            "Query: what is the population of Morocco ?\n",
            "Error processing query: 'FAISS' object has no attribute 'similarity_search_by_vector_with_relevance_scores'\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-859e413b60fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-859e413b60fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "# Configuration class for the RAG system\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # DeepSeek API configuration (replace with your API key)\n",
        "        self.deepseek_api_key = \"sk-c6e6fbf20e3743dc98e51280d199b33d\"\n",
        "        self.deepseek_api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
        "\n",
        "        # DeepSeek embedding model\n",
        "        self.embedding_model_name = \"deepseek-ai/deepseek-coder-7b-base\"\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 5\n",
        "        self.thinking_steps = True\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding\n",
        "class DeepSeekEmbedding:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(config.embedding_model_name)\n",
        "\n",
        "    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and chunking documents\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "\n",
        "    def load_documents(self, directory_path: str) -> List[Document]:\n",
        "        \"\"\"Load documents from a directory.\"\"\"\n",
        "        # Configure loaders for different file types\n",
        "        loaders = {\n",
        "            \"*.txt\": TextLoader,\n",
        "            \"*.pdf\": PDFMinerLoader,\n",
        "            \"*.md\": UnstructuredMarkdownLoader,\n",
        "            \"*.csv\": CSVLoader\n",
        "        }\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        for glob_pattern, loader_cls in loaders.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(\n",
        "                    directory_path,\n",
        "                    glob=glob_pattern,\n",
        "                    loader_cls=loader_cls\n",
        "                )\n",
        "                docs = loader.load()\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {glob_pattern} documents: {e}\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: DeepSeekEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        metadatas = [doc.metadata for doc in documents]\n",
        "\n",
        "        embeddings = self.embedding_model.get_embeddings(texts)\n",
        "\n",
        "        self.vector_store = FAISS.from_embeddings(\n",
        "            text_embeddings=zip(texts, embeddings),\n",
        "            embedding=self.embedding_model,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "\n",
        "        # Save the vector store\n",
        "        self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        if os.path.exists(self.config.vector_db_path):\n",
        "            self.vector_store = FAISS.load_local(\n",
        "                self.config.vector_db_path,\n",
        "                self.embedding_model\n",
        "            )\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        query_embedding = self.embedding_model.get_embeddings([query])[0]\n",
        "\n",
        "        results = self.vector_store.similarity_search_by_vector_with_relevance_scores(\n",
        "            query_embedding,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if score >= self.config.similarity_threshold\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# DeepSeek LLM client for RAG\n",
        "class DeepSeekLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {config.deepseek_api_key}\"\n",
        "        }\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using DeepSeek API.\"\"\"\n",
        "        messages = []\n",
        "\n",
        "        if system_message:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"deepseek-chat\",\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.config.deepseek_api_url,\n",
        "                headers=self.headers,\n",
        "                json=data\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling DeepSeek API: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = DeepSeekEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = DeepSeekLLM(config)\n",
        "\n",
        "    def ingest_documents(self, directory_path: str) -> None:\n",
        "        \"\"\"Ingest documents from a directory.\"\"\"\n",
        "        print(f\"Loading documents from {directory_path}...\")\n",
        "        documents = self.document_processor.load_documents(directory_path)\n",
        "        print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "        print(\"Processing documents...\")\n",
        "        chunks = self.document_processor.process_documents(documents)\n",
        "        print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"Creating vector store...\")\n",
        "        self.vector_store.create_vector_store(chunks)\n",
        "        print(\"Vector store created successfully.\")\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a user query and generate a response.\"\"\"\n",
        "        # Try to load the vector store if not already loaded\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "\n",
        "        # Initial response\n",
        "        print(f\"Query: {user_query}\")\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "\n",
        "# Command-line interface for the AgenticRAG system\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with DeepSeek\")\n",
        "        print(\"1. Ingest documents\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            directory = input(\"Enter the directory path containing your documents: \")\n",
        "            try:\n",
        "                rag_system.ingest_documents(directory)\n",
        "            except Exception as e:\n",
        "                print(f\"Error ingesting documents: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with DeepSeek. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "23bccefbbb904d4b9a194a38f9c47bd0",
            "8b08cfb48f1f4694b46fc15cbd7f3685",
            "be8f50dd7e704a0c9412c125024acd3f",
            "86ff4c24094c4fcab6d868ef13914bc0",
            "d8573df9ea154f28a2293b0c98295360",
            "781c3342a7184f3db2742735a40c56d4",
            "849ef973f82445039fc1ab822a20d0b2",
            "3f9974e739fe4482888a27267fd39869",
            "b927ed0f4d474e48bb1a76a97890d30c",
            "1b89330dea1f4b3cb6231a51f6d389f3",
            "715718e9bfaa45e5a0003ee8c0edf009",
            "9aa1ab73daf14df3a9f2319dc3c00903",
            "089c898cdb61490fb9f94c96243808cf",
            "e1b35ac7557c4d52a2d48a8bedba6eb9",
            "a70a0fb9d9ec4acabba6292b56db3257",
            "b677b73587cc47c983bfab42565cc961",
            "c762d263b5d74699b8a8aa986220a846",
            "de2d73886a0b40ff89bc3aa3536ec1bc",
            "5bf33933c4844b0685902f778c9cb851",
            "10a63cbadf724f10b7199f28cacc2fee",
            "6fa4f21eef934c8ea0008844ec337726",
            "de6b3b552b8e4a428e0353eee3a84fdd",
            "4eb61d2861c14d4abc2e8a79bffcf069",
            "6995a35f3ef3473db8a8e43a4e0c35ef",
            "c4098ab358384460b1022e609af11d10",
            "6063b77726af4f4cbfdfb7f0f82439b0",
            "ad9e4a822e784210b51de4e47af45ff1",
            "a0031ab68e064419bf8773d2acc35be1",
            "4801eb1ea7ae4b7f92376b849c47b825",
            "4b12394770ea44bd90bdda009c194f4f",
            "49ae0e87ed82480585d9ff6265fee9c5",
            "afebecf506b94d929445fa7cc9ce7f6f",
            "262a6f7bf8664ceabbffbbf43a7e6fc6",
            "c2f4de0956b1496c88fc002617c3ad04",
            "c906a5fd23bc492daf94360598b8df67",
            "00d2f7dc5c5e48ae988615dfce6660f0",
            "b0aeac7f134744478eeb1bdd3bc9d204",
            "48d859d1d0de419a8db57357bc30751a",
            "6471bf6059b54433965077fef4c7122d",
            "20f3be16a8a1403aa3535752f65aa6d2",
            "4833e625da3b432fa8c58f701c6fb79d",
            "7a8c3ad80f954d0da18809d60bf0842d",
            "781bfd1107354bb8acb507f5b05ed791",
            "376b74a25e394b669ed4a5a862e31b94",
            "72396654f2f14554836b378b5718b1ba",
            "0fd45d091b644a838e99b1f54cb7b2dd",
            "7cc590b7765d4e21b56e7ef96b4dc7a4",
            "45963d3c22ff4696ab344baf99c26f39",
            "2dc35c24871b4d6f9fb8ab1d5968d46e",
            "6718531a8a1a4e0894a12aee2d321b05",
            "fe8991a0594746cd83ff681f13db9318",
            "7f714f06b74c4359a96c64856c5cacd6",
            "cbb2c608fd484dcc81555c1e27def387",
            "cd6589f04de84e51b382838ae34fdc2b",
            "dc03f6a851ca424cad5b38c75f204a8a",
            "28dd8ba74823470dbca2a8984cb60416",
            "3e57e0e9c6dc46f7920abf80b488b0fe",
            "97ccfa9d1107401397592870090ac065",
            "31685198b525456ea1885759492cb155",
            "a95a5441f3c34917b60888beba143bfe",
            "2add0ab6ed374b858f6f2511205aa74c",
            "32df62147db0465fb0ee160650d32a1a",
            "e322e6f42c904b86a39510fa48bbdccb",
            "8740c537426743a3ae136c7348496a5b",
            "a1eb5e1262d84f2b831029abc38f2a2c",
            "10ee5da44cbc400abaf63fbb8e88bd5b",
            "c773318ac9914b40886b5946e9eb9863",
            "13efe2d32b9445118a3ccd4346d6c585",
            "0d57fb7c0531470eb9b2a3a291b9ebd9",
            "9983dcb363f9414f824014ad8648768d",
            "a8127533eeca44a99e300dad7d6f88e4",
            "e90216632f754eefb2f6d5a7fc63c316",
            "a0bb6cab2b784cde91694f43178bbd1f",
            "4eb0051de4b44db2a46bb9a6d2b470a4",
            "3b00caf26f66472bb3e15e160b9f1e6c",
            "ccb1573215334885a9262ab960c19f23",
            "f748c9d935b14ff2b47a95a83dbfccc0",
            "df3c24cb37544ef389742edf307df828",
            "009dc548f27b478fb83cbbd74c4f5254",
            "17f55230b71a4273acfd49224773442a",
            "4b0e78aa810f4479a5da72f229231ead",
            "4f638fb86d774ecaa3408c83eec1b543",
            "119e27240c334322b0bd8b25594d4452",
            "e9fef4bdad4a43a18fabff54a2c51c57",
            "3713ef908e02410b850dd67848575076",
            "534e73f8fd1944caa539c3caf3b8e2a2",
            "27399dd450a84ef985de553fc85f9d3c",
            "91a8a324e3094afe8d5922f52d40dc8a"
          ]
        },
        "id": "2LORT6XYKkoe",
        "outputId": "dba1cae7-1e2f-454b-996d-4cbd9bceeeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23bccefbbb904d4b9a194a38f9c47bd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aa1ab73daf14df3a9f2319dc3c00903"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eb61d2861c14d4abc2e8a79bffcf069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2f4de0956b1496c88fc002617c3ad04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72396654f2f14554836b378b5718b1ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28dd8ba74823470dbca2a8984cb60416"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c773318ac9914b40886b5946e9eb9863"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df3c24cb37544ef389742edf307df828"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter the directory path containing your documents: /content/Document\n",
            "Loading documents from /content/Document...\n",
            "Loaded 1 documents.\n",
            "Processing documents...\n",
            "Created 4 chunks.\n",
            "Creating vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created successfully.\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: hi\n",
            "Query: hi\n",
            "Error processing query: 'DeepSeekEmbedding' object is not callable\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-454484518f35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-454484518f35>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "# Configuration class for the RAG system\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # DeepSeek API configuration (replace with your API key)\n",
        "        self.deepseek_api_key = \"sk-c6e6fbf20e3743dc98e51280d199b33d\"\n",
        "        self.deepseek_api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
        "\n",
        "        # DeepSeek embedding model\n",
        "        self.embedding_model_name = \"deepseek-ai/deepseek-coder-7b-base\"\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 5\n",
        "        self.thinking_steps = True\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding\n",
        "class DeepSeekEmbedding:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(config.embedding_model_name)\n",
        "\n",
        "    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and chunking documents\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "\n",
        "    def load_documents(self, directory_path: str) -> List[Document]:\n",
        "        \"\"\"Load documents from a directory.\"\"\"\n",
        "        # Configure loaders for different file types\n",
        "        loaders = {\n",
        "            \"*.txt\": TextLoader,\n",
        "            \"*.pdf\": PDFMinerLoader,\n",
        "            \"*.md\": UnstructuredMarkdownLoader,\n",
        "            \"*.csv\": CSVLoader\n",
        "        }\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        for glob_pattern, loader_cls in loaders.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(\n",
        "                    directory_path,\n",
        "                    glob=glob_pattern,\n",
        "                    loader_cls=loader_cls\n",
        "                )\n",
        "                docs = loader.load()\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {glob_pattern} documents: {e}\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: DeepSeekEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        metadatas = [doc.metadata for doc in documents]\n",
        "\n",
        "        embeddings = self.embedding_model.get_embeddings(texts)\n",
        "\n",
        "        self.vector_store = FAISS.from_embeddings(\n",
        "            text_embeddings=zip(texts, embeddings),\n",
        "            embedding=self.embedding_model,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "\n",
        "        # Save the vector store\n",
        "        self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        if os.path.exists(self.config.vector_db_path):\n",
        "            self.vector_store = FAISS.load_local(\n",
        "                self.config.vector_db_path,\n",
        "                self.embedding_model\n",
        "            )\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Replace the similarity_search method in the VectorStore class with this:\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use the standard similarity_search method instead\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: FAISS returns distance, not similarity score, so we need to convert\n",
        "        # FAISS distance is lower for more similar items\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# DeepSeek LLM client for RAG\n",
        "class DeepSeekLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {config.deepseek_api_key}\"\n",
        "        }\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using DeepSeek API.\"\"\"\n",
        "        messages = []\n",
        "\n",
        "        if system_message:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"deepseek-chat\",\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.config.deepseek_api_url,\n",
        "                headers=self.headers,\n",
        "                json=data\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling DeepSeek API: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = DeepSeekEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = DeepSeekLLM(config)\n",
        "\n",
        "    def ingest_documents(self, directory_path: str) -> None:\n",
        "        \"\"\"Ingest documents from a directory.\"\"\"\n",
        "        print(f\"Loading documents from {directory_path}...\")\n",
        "        documents = self.document_processor.load_documents(directory_path)\n",
        "        print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "        print(\"Processing documents...\")\n",
        "        chunks = self.document_processor.process_documents(documents)\n",
        "        print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"Creating vector store...\")\n",
        "        self.vector_store.create_vector_store(chunks)\n",
        "        print(\"Vector store created successfully.\")\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a user query and generate a response.\"\"\"\n",
        "        # Try to load the vector store if not already loaded\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "\n",
        "        # Initial response\n",
        "        print(f\"Query: {user_query}\")\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "\n",
        "# Command-line interface for the AgenticRAG system\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with DeepSeek\")\n",
        "        print(\"1. Ingest documents\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            directory = input(\"Enter the directory path containing your documents: \")\n",
        "            try:\n",
        "                rag_system.ingest_documents(directory)\n",
        "            except Exception as e:\n",
        "                print(f\"Error ingesting documents: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with DeepSeek. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "# Configuration class for the RAG system\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # DeepSeek API configuration (replace with your API key)\n",
        "        self.deepseek_api_key = \"sk-c6e6fbf20e3743dc98e51280d199b33d\"\n",
        "        self.deepseek_api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
        "\n",
        "        # DeepSeek embedding model\n",
        "        self.embedding_model_name = \"deepseek-ai/deepseek-coder-7b-base\"\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 5\n",
        "        self.thinking_steps = True\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class DeepSeekEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(config.embedding_model_name)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and chunking documents\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "\n",
        "    def load_documents(self, directory_path: str) -> List[Document]:\n",
        "        \"\"\"Load documents from a directory.\"\"\"\n",
        "        # Configure loaders for different file types\n",
        "        loaders = {\n",
        "            \"*.txt\": TextLoader,\n",
        "            \"*.pdf\": PDFMinerLoader,\n",
        "            \"*.md\": UnstructuredMarkdownLoader,\n",
        "            \"*.csv\": CSVLoader\n",
        "        }\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        for glob_pattern, loader_cls in loaders.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(\n",
        "                    directory_path,\n",
        "                    glob=glob_pattern,\n",
        "                    loader_cls=loader_cls\n",
        "                )\n",
        "                docs = loader.load()\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {glob_pattern} documents: {e}\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: DeepSeekEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use the FAISS.from_documents method which properly handles Embeddings objects\n",
        "        self.vector_store = FAISS.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model\n",
        "        )\n",
        "\n",
        "        # Save the vector store\n",
        "        self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        if os.path.exists(self.config.vector_db_path):\n",
        "            self.vector_store = FAISS.load_local(\n",
        "                self.config.vector_db_path,\n",
        "                self.embedding_model\n",
        "            )\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use the standard similarity_search method instead\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: FAISS returns distance, not similarity score, so we need to convert\n",
        "        # FAISS distance is lower for more similar items\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# DeepSeek LLM client for RAG\n",
        "class DeepSeekLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {config.deepseek_api_key}\"\n",
        "        }\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using DeepSeek API.\"\"\"\n",
        "        messages = []\n",
        "\n",
        "        if system_message:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"deepseek-chat\",\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.config.deepseek_api_url,\n",
        "                headers=self.headers,\n",
        "                json=data\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling DeepSeek API: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = DeepSeekEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = DeepSeekLLM(config)\n",
        "\n",
        "    def ingest_documents(self, directory_path: str) -> None:\n",
        "        \"\"\"Ingest documents from a directory.\"\"\"\n",
        "        print(f\"Loading documents from {directory_path}...\")\n",
        "        documents = self.document_processor.load_documents(directory_path)\n",
        "        print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "        print(\"Processing documents...\")\n",
        "        chunks = self.document_processor.process_documents(documents)\n",
        "        print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"Creating vector store...\")\n",
        "        self.vector_store.create_vector_store(chunks)\n",
        "        print(\"Vector store created successfully.\")\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a user query and generate a response.\"\"\"\n",
        "        # Try to load the vector store if not already loaded\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "\n",
        "        # Initial response\n",
        "        print(f\"Query: {user_query}\")\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "\n",
        "# Command-line interface for the AgenticRAG system\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with DeepSeek\")\n",
        "        print(\"1. Ingest documents\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            directory = input(\"Enter the directory path containing your documents: \")\n",
        "            try:\n",
        "                rag_system.ingest_documents(directory)\n",
        "            except Exception as e:\n",
        "                print(f\"Error ingesting documents: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with DeepSeek. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913,
          "referenced_widgets": [
            "5e1d6682b7c64cf49b1867896a793dec",
            "9faffccf75f94d7bbaeaca82f26c6f1d",
            "0a99a5afce9d409fad5e77a7010d697c",
            "af3cd7b57f384959adc8cda78a0213e4",
            "93b235f9be8e45dcb5c11f8d814fd5e4",
            "c19cc91a69e7452ea6c36631e5735487",
            "2062979197694d4bba591641bcbc8c07",
            "e4299877fa0345478680b34ac307a3f3",
            "c669a243af274684948c0cb15ac10924",
            "6e73f0126660404faddddcf997fc9ec2",
            "cb64e53a7a094560a5f1c3145250b970"
          ]
        },
        "id": "RfPfPOAs0y5i",
        "outputId": "ab7ee14b-1196-4bc8-fd68-ea9f34c17516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e1d6682b7c64cf49b1867896a793dec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter the directory path containing your documents: /content/Document\n",
            "Loading documents from /content/Document...\n",
            "Loaded 1 documents.\n",
            "Processing documents...\n",
            "Created 4 chunks.\n",
            "Creating vector store...\n",
            "Vector store created successfully.\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: what is the capital of moroco ?\n",
            "Query: what is the capital of moroco ?\n",
            "Error calling DeepSeek API: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
            "\n",
            "Response (took 12.90 seconds):\n",
            "Error generating response: 402 Client Error: Payment Required for url: https://api.deepseek.com/v1/chat/completions\n",
            "\n",
            "AgenticRAG with DeepSeek\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8eeb96f2a0bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-8eeb96f2a0bf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "# Configuration class for the RAG system\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and chunking documents\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "\n",
        "    def load_documents(self, directory_path: str) -> List[Document]:\n",
        "        \"\"\"Load documents from a directory.\"\"\"\n",
        "        # Configure loaders for different file types\n",
        "        loaders = {\n",
        "            \"*.txt\": TextLoader,\n",
        "            \"*.pdf\": PDFMinerLoader,\n",
        "            \"*.md\": UnstructuredMarkdownLoader,\n",
        "            \"*.csv\": CSVLoader\n",
        "        }\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        for glob_pattern, loader_cls in loaders.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(\n",
        "                    directory_path,\n",
        "                    glob=glob_pattern,\n",
        "                    loader_cls=loader_cls\n",
        "                )\n",
        "                docs = loader.load()\n",
        "                documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {glob_pattern} documents: {e}\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use the FAISS.from_documents method which properly handles Embeddings objects\n",
        "        self.vector_store = FAISS.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model\n",
        "        )\n",
        "\n",
        "        # Save the vector store\n",
        "        self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        if os.path.exists(self.config.vector_db_path):\n",
        "            self.vector_store = FAISS.load_local(\n",
        "                self.config.vector_db_path,\n",
        "                self.embedding_model\n",
        "            )\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use the standard similarity_search method instead\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: FAISS returns distance, not similarity score, so we need to convert\n",
        "        # FAISS distance is lower for more similar items\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=1000,  # Reduced for faster generation\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "\n",
        "    def ingest_documents(self, directory_path: str) -> None:\n",
        "        \"\"\"Ingest documents from a directory.\"\"\"\n",
        "        print(f\"Loading documents from {directory_path}...\")\n",
        "        documents = self.document_processor.load_documents(directory_path)\n",
        "        print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "        print(\"Processing documents...\")\n",
        "        chunks = self.document_processor.process_documents(documents)\n",
        "        print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"Creating vector store...\")\n",
        "        self.vector_store.create_vector_store(chunks)\n",
        "        print(\"Vector store created successfully.\")\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a user query and generate a response.\"\"\"\n",
        "        # Try to load the vector store if not already loaded\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "\n",
        "        # Initial response\n",
        "        print(f\"Query: {user_query}\")\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations}\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "\n",
        "# Command-line interface for the AgenticRAG system\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing AgenticRAG with models on {config.device}\")\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with Local LLM\")\n",
        "        print(\"1. Ingest documents\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            directory = input(\"Enter the directory path containing your documents: \")\n",
        "            try:\n",
        "                rag_system.ingest_documents(directory)\n",
        "            except Exception as e:\n",
        "                print(f\"Error ingesting documents: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with Local LLM. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "nQCBcMg35H7L",
        "outputId": "3fc6974c-391d-4902-dc2c-9cbea54d5b7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AgenticRAG with models on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "\n",
            "AgenticRAG with Local LLM\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter the directory path containing your documents: /content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf\n",
            "Loading documents from /content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf...\n",
            "Error loading *.txt documents: Expected directory, got file: '/content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf'\n",
            "Error loading *.pdf documents: Expected directory, got file: '/content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf'\n",
            "Error loading *.md documents: Expected directory, got file: '/content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf'\n",
            "Error loading *.csv documents: Expected directory, got file: '/content/c._r._pennell_morocco_from_empire_to_independenbook4me.org_.pdf'\n",
            "Loaded 0 documents.\n",
            "Processing documents...\n",
            "Created 0 chunks.\n",
            "Creating vector store...\n",
            "Error ingesting documents: list index out of range\n",
            "\n",
            "AgenticRAG with Local LLM\n",
            "1. Ingest documents\n",
            "2. Ask a question\n",
            "3. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-04e6043a5325>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-04e6043a5325>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import tempfile\n",
        "from typing import List, Dict, Any, Optional, Tuple, BinaryIO\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "# Configuration class for the RAG system\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        # Local model configuration - using publicly available models\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Public embedding model\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Public LLM model\n",
        "\n",
        "        # Model parameters\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Vector database configuration\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "\n",
        "        # Document processing configuration\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "\n",
        "        # Search configuration\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "\n",
        "        # Agent configuration\n",
        "        self.max_iterations = 3  # Reduced for faster execution\n",
        "        self.thinking_steps = True\n",
        "\n",
        "\n",
        "# Embedding class for document and query encoding - implement Embeddings interface\n",
        "class SentenceTransformerEmbedding(Embeddings):\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading embedding model {config.embedding_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model_name)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            config.embedding_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self._get_embeddings(texts)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for a query string.\"\"\"\n",
        "        embeddings = self._get_embeddings([text])\n",
        "        return embeddings[0]\n",
        "\n",
        "    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Internal method to generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.config.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Document processor for loading and processing uploaded files\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap\n",
        "        )\n",
        "        self.temp_dir = None\n",
        "\n",
        "    def process_uploaded_file(self, file_obj: BinaryIO, filename: str) -> List[Document]:\n",
        "        \"\"\"Process a single uploaded file.\"\"\"\n",
        "        # Create a temporary directory if not already created\n",
        "        if self.temp_dir is None:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Get file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        # Save the file to the temporary directory\n",
        "        temp_file_path = os.path.join(self.temp_dir, filename)\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            f.write(file_obj.read())\n",
        "\n",
        "        # Select appropriate loader based on file extension\n",
        "        loader = None\n",
        "        if file_extension == '.txt':\n",
        "            loader = TextLoader(temp_file_path)\n",
        "        elif file_extension == '.pdf':\n",
        "            loader = PDFMinerLoader(temp_file_path)\n",
        "        elif file_extension == '.md':\n",
        "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
        "        elif file_extension == '.csv':\n",
        "            loader = CSVLoader(temp_file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        # Load and process the document\n",
        "        documents = loader.load()\n",
        "        chunks = self.process_documents(documents)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks for embedding.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                doc_chunks = self.text_splitter.split_documents([doc])\n",
        "                chunks.extend(doc_chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {doc.metadata.get('source', 'unknown')}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Remove temporary files when done.\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "            self.temp_dir = None\n",
        "\n",
        "\n",
        "# Vector store for document storage and retrieval\n",
        "class VectorStore:\n",
        "    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformerEmbedding):\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vector_store = None\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Create a vector store from documents.\"\"\"\n",
        "        # Use the FAISS.from_documents method which properly handles Embeddings objects\n",
        "        self.vector_store = FAISS.from_documents(\n",
        "            documents,\n",
        "            self.embedding_model\n",
        "        )\n",
        "\n",
        "        # Save the vector store\n",
        "        self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def add_documents(self, documents: List[Document]) -> None:\n",
        "        \"\"\"Add documents to an existing vector store.\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            # If no vector store exists, create a new one\n",
        "            self.create_vector_store(documents)\n",
        "        else:\n",
        "            # Add documents to existing vector store\n",
        "            self.vector_store.add_documents(documents)\n",
        "            # Save the updated vector store\n",
        "            self.vector_store.save_local(self.config.vector_db_path)\n",
        "\n",
        "    def load_vector_store(self) -> bool:\n",
        "        \"\"\"Load the vector store if it exists.\"\"\"\n",
        "        if os.path.exists(self.config.vector_db_path):\n",
        "            self.vector_store = FAISS.load_local(\n",
        "                self.config.vector_db_path,\n",
        "                self.embedding_model\n",
        "            )\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def similarity_search(self, query: str) -> List[Document]:\n",
        "        \"\"\"Search for similar documents to the query.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Vector store not initialized. Please create or load a vector store first.\")\n",
        "\n",
        "        # Use the standard similarity_search method instead\n",
        "        results = self.vector_store.similarity_search_with_score(\n",
        "            query,\n",
        "            k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        # Filter results by similarity threshold\n",
        "        # Note: FAISS returns distance, not similarity score, so we need to convert\n",
        "        # FAISS distance is lower for more similar items\n",
        "        filtered_results = [\n",
        "            doc for doc, score in results\n",
        "            if 1.0 / (1.0 + score) >= self.config.similarity_threshold  # Convert distance to similarity\n",
        "        ]\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "\n",
        "# Local LLM for RAG\n",
        "class LocalLLM:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        print(f\"Loading LLM model {config.llm_model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.llm_model_name,\n",
        "            torch_dtype=config.torch_dtype\n",
        "        ).to(config.device)\n",
        "        print(f\"LLM model loaded successfully on {config.device}\")\n",
        "\n",
        "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
        "        \"\"\"Generate text using local model.\"\"\"\n",
        "        # Format the messages\n",
        "        if system_message:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        try:\n",
        "            # Format messages for chat format\n",
        "            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        except Exception as e:\n",
        "            # Fallback if the model doesn't support chat templates\n",
        "            print(f\"Chat template error: {e}, using simple prompt formatting\")\n",
        "            formatted_prompt = system_message + \"\\n\\n\" + prompt if system_message else prompt\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=1000,  # Reduced for faster generation\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode the output, removing the input tokens\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        response_tokens = outputs[0][input_length:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "# Agentic RAG system that combines all components\n",
        "class AgenticRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.document_processor = DocumentProcessor(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "\n",
        "    def ingest_uploaded_file(self, file_obj: BinaryIO, filename: str) -> None:\n",
        "        \"\"\"Ingest a single uploaded file.\"\"\"\n",
        "        print(f\"Processing uploaded file: {filename}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the uploaded file\n",
        "            chunks = self.document_processor.process_uploaded_file(file_obj, filename)\n",
        "            print(f\"Created {len(chunks)} chunks from {filename}.\")\n",
        "\n",
        "            # Try to load the vector store first\n",
        "            vector_store_exists = self.vector_store.load_vector_store()\n",
        "\n",
        "            if vector_store_exists:\n",
        "                # Add the new documents to the existing vector store\n",
        "                print(\"Adding documents to existing vector store...\")\n",
        "                self.vector_store.add_documents(chunks)\n",
        "            else:\n",
        "                # Create a new vector store if none exists\n",
        "                print(\"Creating new vector store...\")\n",
        "                self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "            print(f\"Successfully ingested {filename}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error ingesting file {filename}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query(self, user_query: str) -> str:\n",
        "        \"\"\"Process a user query and generate a response.\"\"\"\n",
        "        # Try to load the vector store if not already loaded\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "\n",
        "        # Initial response\n",
        "        print(f\"Query: {user_query}\")\n",
        "\n",
        "        # Agentic thinking process\n",
        "        response = self._agentic_process(user_query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _agentic_process(self, user_query: str) -> str:\n",
        "        \"\"\"Execute the agentic process for responding to queries.\"\"\"\n",
        "        system_message = \"\"\"You are an intelligent agent with access to a knowledge base.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        Think step by step and analyze the retrieved information carefully before formulating your final response.\"\"\"\n",
        "\n",
        "        # Initial retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(user_query)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            # Handle the case when no relevant documents are found\n",
        "            prompt = f\"\"\"Query: {user_query}\n",
        "\n",
        "            No relevant documents were found in the knowledge base. Please provide a general response based on your knowledge.\n",
        "            \"\"\"\n",
        "            return self.llm.generate(prompt, system_message)\n",
        "\n",
        "        # For agentic reasoning, we'll use a multi-step process\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        iteration_responses = []\n",
        "        current_query = user_query\n",
        "\n",
        "        for iteration in range(self.config.max_iterations):\n",
        "            print(f\"Iteration {iteration+1}/{self.config.max_iterations}\")\n",
        "\n",
        "            # Check if we should continue\n",
        "            if iteration > 0 and not self._should_continue(current_query, iteration_responses[-1]):\n",
        "                break\n",
        "\n",
        "            # Generate thinking steps if enabled\n",
        "            thinking = \"\"\n",
        "            if self.config.thinking_steps:\n",
        "                thinking_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Think step by step about this query. What are the key points to address? What information from the context is most relevant? What additional information might be needed?\n",
        "                \"\"\"\n",
        "                thinking = self.llm.generate(thinking_prompt, system_message)\n",
        "\n",
        "            # Generate the response\n",
        "            response_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            {thinking if thinking else \"\"}\n",
        "\n",
        "            Based on the context provided, please answer the query. If the context doesn't contain enough information, acknowledge this and provide the best answer you can.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.llm.generate(response_prompt, system_message)\n",
        "            iteration_responses.append(response)\n",
        "\n",
        "            # Generate follow-up questions or refinements\n",
        "            refinement_prompt = f\"\"\"Query: {current_query}\n",
        "\n",
        "            Your current response:\n",
        "            {response}\n",
        "\n",
        "            Are there aspects of the query that haven't been fully addressed? What follow-up questions would help provide a more complete answer? How could the search be refined?\n",
        "            \"\"\"\n",
        "\n",
        "            refinement = self.llm.generate(refinement_prompt, system_message)\n",
        "\n",
        "            # Extract a new query for the next iteration\n",
        "            new_query_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "            Current response:\n",
        "            {response}\n",
        "\n",
        "            Refinement thoughts:\n",
        "            {refinement}\n",
        "\n",
        "            Based on the above, formulate a new search query that would help address any gaps in the current response. Return ONLY the new query without any explanation.\n",
        "            If you believe the query has been fully addressed, return \"COMPLETE\".\n",
        "            \"\"\"\n",
        "\n",
        "            new_query = self.llm.generate(new_query_prompt, system_message).strip()\n",
        "\n",
        "            if new_query == \"COMPLETE\" or new_query.upper().startswith(\"COMPLETE\"):\n",
        "                break\n",
        "\n",
        "            # Perform a new search with the refined query\n",
        "            current_query = new_query\n",
        "            new_docs = self.vector_store.similarity_search(current_query)\n",
        "\n",
        "            if new_docs:\n",
        "                new_context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(new_docs)])\n",
        "                # Update context with new information\n",
        "                context = f\"{context}\\n\\nAdditional Context:\\n{new_context}\"\n",
        "\n",
        "        # Final synthesis\n",
        "        final_prompt = f\"\"\"Original query: {user_query}\n",
        "\n",
        "        Iterations of responses:\n",
        "        {' '.join([f\"Iteration {i+1}: {resp}\" for i, resp in enumerate(iteration_responses)])}\n",
        "\n",
        "        Please provide a final, comprehensive response to the original query that synthesizes all the information gathered across iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = self.llm.generate(final_prompt, system_message)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _should_continue(self, query: str, last_response: str) -> bool:\n",
        "        \"\"\"Determine if the agent should continue iterating.\"\"\"\n",
        "        prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Current response:\n",
        "        {last_response}\n",
        "\n",
        "        Does this response fully address the query? If yes, respond with \"COMPLETE\". If not, respond with \"CONTINUE\" and briefly explain why.\n",
        "        \"\"\"\n",
        "\n",
        "        decision = self.llm.generate(prompt)\n",
        "        return \"CONTINUE\" in decision.upper()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files.\"\"\"\n",
        "        self.document_processor.cleanup()\n",
        "\n",
        "\n",
        "# Google Colab integration for file upload and RAG system\n",
        "def run_in_colab():\n",
        "    from google.colab import files\n",
        "    import io\n",
        "\n",
        "    config = RAGConfig()\n",
        "    print(f\"Initializing AgenticRAG with models on {config.device}\")\n",
        "    rag_system = AgenticRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAgenticRAG with Local LLM\")\n",
        "        print(\"1. Upload and ingest a file\")\n",
        "        print(\"2. Ask a question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            try:\n",
        "                print(\"Please select a file to upload...\")\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                for filename, content in uploaded.items():\n",
        "                    file_obj = io.BytesIO(content)\n",
        "                    rag_system.ingest_uploaded_file(file_obj, filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading and ingesting file: {e}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your question: \")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = rag_system.query(query)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(f\"\\nResponse (took {end_time - start_time:.2f} seconds):\")\n",
        "                print(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {e}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using AgenticRAG with Local LLM. Goodbye!\")\n",
        "            # Clean up any temporary files\n",
        "            rag_system.cleanup()\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "# For standalone execution outside of Colab\n",
        "def main():\n",
        "    try:\n",
        "        # Check if running in Google Colab\n",
        "        import google.colab\n",
        "        print(\"Running in Google Colab environment\")\n",
        "        run_in_colab()\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab, falling back to command-line interface\")\n",
        "        # Original command-line interface code here\n",
        "        # (You can keep the original code if needed, but it's not the focus now)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oNjLhDogE-5b",
        "outputId": "12a9e0ea-e6c0-4d08-c0a6-5e5f0a032001"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Google Colab environment\n",
            "Initializing AgenticRAG with models on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "\n",
            "AgenticRAG with Local LLM\n",
            "1. Upload and ingest a file\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Please select a file to upload...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e92b8ee-33d4-401f-9806-7916071d7266\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e92b8ee-33d4-401f-9806-7916071d7266\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 1706.03762v7 (1).pdf to 1706.03762v7 (1).pdf\n",
            "Processing uploaded file: 1706.03762v7 (1).pdf...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 52 chunks from 1706.03762v7 (1).pdf.\n",
            "Creating new vector store...\n",
            "Successfully ingested 1706.03762v7 (1).pdf.\n",
            "\n",
            "AgenticRAG with Local LLM\n",
            "1. Upload and ingest a file\n",
            "2. Ask a question\n",
            "3. Exit\n",
            "Query: what is attention\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response (took 360.28 seconds):\n",
            "<|assistant|>\n",
            "Attention is a crucial cognitive function that helps us process and interpret information. When we encounter a new piece of information, our brains scan it for relevant details to help us understand its meaning. For example, if we receive an email with a detailed product description, our attention is captured and we focus on details such as the product name, pictures, and price.\n",
            "\n",
            "In this scenario, your agent should respond by providing an overview of the product description, such as the name, brand, features, and price. A general response might include something like, \"that's a great description of the product. Would you like to know more about its features and price?\"\n",
            "\n",
            "Your agent should think carefully about the context of the query and the retrieved information to provide a more detailed response. You could also ask follow-up questions to clarify the information provided by the context. For example, \"Would you like to know more about the product's dimensions and weight? Or, could you provide some information on the manufacturer's reputation in this industry?\"\n",
            "\n",
            "Remember, the key is to provide accurate, relevant information, avoiding irrelevant or false information. Your response should be concise, clear, and free of errors.\n",
            "\n",
            "AgenticRAG with Local LLM\n",
            "1. Upload and ingest a file\n",
            "2. Ask a question\n",
            "3. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-03249d751738>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-03249d751738>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in Google Colab environment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0mrun_in_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not running in Google Colab, falling back to command-line interface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-03249d751738>\u001b[0m in \u001b[0;36mrun_in_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-3): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "# [Previous code for RAGConfig, SentenceTransformerEmbedding, DocumentProcessor, VectorStore, LocalLLM remains the same]\n",
        "\n",
        "# New Multi-Agent Communication System\n",
        "class AgentCommunicationProtocol:\n",
        "    def __init__(self, agents: List['SpecializedAgent']):\n",
        "        self.agents = {agent.name: agent for agent in agents}\n",
        "        self.communication_log = []\n",
        "\n",
        "    def broadcast_message(self, sender: str, message: str, recipients: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Broadcast a message from one agent to others.\n",
        "        If recipients are not specified, message goes to all agents except the sender.\n",
        "        \"\"\"\n",
        "        if not recipients:\n",
        "            recipients = [name for name in self.agents.keys() if name != sender]\n",
        "\n",
        "        for recipient_name in recipients:\n",
        "            if recipient_name in self.agents:\n",
        "                response = self.agents[recipient_name].receive_message(sender, message)\n",
        "                self.communication_log.append({\n",
        "                    'sender': sender,\n",
        "                    'recipient': recipient_name,\n",
        "                    'message': message,\n",
        "                    'response': response\n",
        "                })\n",
        "\n",
        "    def get_communication_history(self):\n",
        "        \"\"\"Retrieve the communication log.\"\"\"\n",
        "        return self.communication_log\n",
        "\n",
        "\n",
        "class SpecializedAgent:\n",
        "    def __init__(self, name: str, domain: str, config: RAGConfig):\n",
        "        \"\"\"\n",
        "        Initialize a specialized agent with a specific domain expertise.\n",
        "\n",
        "        :param name: Unique name of the agent\n",
        "        :param domain: Specialized domain of knowledge\n",
        "        :param config: RAG configuration\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.domain = domain\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "        self.communication_protocol = None\n",
        "\n",
        "    def set_communication_protocol(self, protocol: AgentCommunicationProtocol):\n",
        "        \"\"\"Set the communication protocol for the agent.\"\"\"\n",
        "        self.communication_protocol = protocol\n",
        "\n",
        "    def ingest_documents(self, directory_path: str):\n",
        "        \"\"\"Ingest documents specific to the agent's domain.\"\"\"\n",
        "        document_processor = DocumentProcessor(self.config)\n",
        "        documents = document_processor.load_documents(directory_path)\n",
        "        chunks = document_processor.process_documents(documents)\n",
        "        self.vector_store.create_vector_store(chunks)\n",
        "\n",
        "    def query(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Process a query within the agent's domain,\n",
        "        with optional inter-agent communication.\n",
        "        \"\"\"\n",
        "        # If no vector store is loaded, try to load from path\n",
        "        if not self.vector_store.vector_store:\n",
        "            if not self.vector_store.load_vector_store():\n",
        "                return f\"{self.name}: No documents have been ingested in my domain.\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # Prepare system message with domain context\n",
        "        system_message = f\"\"\"You are the {self.name} agent, specialized in {self.domain} domain.\n",
        "        Your task is to provide accurate, relevant information based on the query and the retrieved context.\n",
        "        If you lack sufficient information, you may request help from other specialized agents.\"\"\"\n",
        "\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "        # First attempt to answer\n",
        "        initial_response_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Based on the context provided, answer the query from the perspective of a {self.domain} expert.\n",
        "        Identify if you need additional expertise to fully address the query.\n",
        "        \"\"\"\n",
        "\n",
        "        initial_response = self.llm.generate(initial_response_prompt, system_message)\n",
        "\n",
        "        # Check if inter-agent communication is needed\n",
        "        need_help_prompt = f\"\"\"Domain: {self.domain}\n",
        "        Query: {query}\n",
        "        Current Response: {initial_response}\n",
        "\n",
        "        Determine if this response requires expertise from other domains.\n",
        "        If additional domain expertise is needed, specify which domains would help.\n",
        "        Respond with a comma-separated list of domain names, or \"NONE\" if no help is needed.\n",
        "        \"\"\"\n",
        "\n",
        "        needed_domains = self.llm.generate(need_help_prompt, system_message).strip()\n",
        "\n",
        "        # Inter-agent communication\n",
        "        if needed_domains.upper() != \"NONE\" and self.communication_protocol:\n",
        "            needed_domains_list = [domain.strip() for domain in needed_domains.split(',')]\n",
        "\n",
        "            # Broadcast message to relevant agents\n",
        "            request_recipients = [\n",
        "                agent_name for agent_name, agent in self.communication_protocol.agents.items()\n",
        "                if agent.domain in needed_domains_list and agent_name != self.name\n",
        "            ]\n",
        "\n",
        "            if request_recipients:\n",
        "                self.communication_protocol.broadcast_message(\n",
        "                    sender=self.name,\n",
        "                    message=f\"Need help with query: {query}. Current context: {initial_response}\",\n",
        "                    recipients=request_recipients\n",
        "                )\n",
        "\n",
        "        return initial_response\n",
        "\n",
        "    def receive_message(self, sender: str, message: str) -> str:\n",
        "        \"\"\"\n",
        "        Handle messages from other agents.\n",
        "\n",
        "        :param sender: Name of the agent sending the message\n",
        "        :param message: Content of the message\n",
        "        :return: Response to the message\n",
        "        \"\"\"\n",
        "        system_message = f\"\"\"You are the {self.name} agent, receiving a message from {sender}.\n",
        "        Analyze the message and provide a helpful response using your domain expertise.\"\"\"\n",
        "\n",
        "        response_prompt = f\"\"\"Message from {sender}:\n",
        "        {message}\n",
        "\n",
        "        Provide a helpful response that leverages your {self.domain} expertise.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.llm.generate(response_prompt, system_message)\n",
        "\n",
        "\n",
        "# Multi-Agent RAG System\n",
        "class MultiAgentRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        \"\"\"\n",
        "        Initialize a multi-agent RAG system with predefined specialized agents.\n",
        "\n",
        "        :param config: RAG configuration\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "        # Create specialized agents\n",
        "        self.agents = [\n",
        "            SpecializedAgent(\"TechAgent\", \"technology and computer science\", config),\n",
        "            SpecializedAgent(\"ScientificAgent\", \"scientific research\", config),\n",
        "            SpecializedAgent(\"BusinessAgent\", \"business and economics\", config),\n",
        "            SpecializedAgent(\"HistoryAgent\", \"history and humanities\", config)\n",
        "        ]\n",
        "\n",
        "        # Set up communication protocol\n",
        "        self.communication_protocol = AgentCommunicationProtocol(self.agents)\n",
        "\n",
        "        # Configure communication for each agent\n",
        "        for agent in self.agents:\n",
        "            agent.set_communication_protocol(self.communication_protocol)\n",
        "\n",
        "    def ingest_documents(self, agent_name: str, directory_path: str):\n",
        "        \"\"\"\n",
        "        Ingest documents for a specific agent.\n",
        "\n",
        "        :param agent_name: Name of the agent\n",
        "        :param directory_path: Path to documents\n",
        "        \"\"\"\n",
        "        agent = next((agent for agent in self.agents if agent.name == agent_name), None)\n",
        "        if agent:\n",
        "            agent.ingest_documents(directory_path)\n",
        "        else:\n",
        "            print(f\"Agent {agent_name} not found.\")\n",
        "\n",
        "    def query(self, query: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Process a query across all agents.\n",
        "\n",
        "        :param query: User query\n",
        "        :return: Responses from different agents\n",
        "        \"\"\"\n",
        "        responses = {}\n",
        "        for agent in self.agents:\n",
        "            response = agent.query(query)\n",
        "            responses[agent.name] = response\n",
        "\n",
        "        return responses\n",
        "\n",
        "    def get_communication_history(self):\n",
        "        \"\"\"\n",
        "        Retrieve the communication history between agents.\n",
        "\n",
        "        :return: List of communication logs\n",
        "        \"\"\"\n",
        "        return self.communication_protocol.get_communication_history()\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    multi_agent_rag = MultiAgentRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMulti-Agent RAG System\")\n",
        "        print(\"1. Ingest documents for an agent\")\n",
        "        print(\"2. Ask a cross-domain question\")\n",
        "        print(\"3. View communication history\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-4): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            agent_name = input(\"Enter agent name (TechAgent, ScientificAgent, BusinessAgent, HistoryAgent): \")\n",
        "            directory = input(\"Enter the directory path containing documents: \")\n",
        "            multi_agent_rag.ingest_documents(agent_name, directory)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            query = input(\"Enter your cross-domain question: \")\n",
        "            responses = multi_agent_rag.query(query)\n",
        "\n",
        "            print(\"\\nResponses from different agents:\")\n",
        "            for agent, response in responses.items():\n",
        "                print(f\"{agent}:\\n{response}\\n\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            history = multi_agent_rag.get_communication_history()\n",
        "            print(\"\\nCommunication History:\")\n",
        "            for log in history:\n",
        "                print(f\"Sender: {log['sender']}, Recipient: {log['recipient']}\")\n",
        "                print(f\"Message: {log['message']}\")\n",
        "                print(f\"Response: {log['response']}\\n\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            print(\"Thank you for using Multi-Agent RAG. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "WjIk6jajS_-u",
        "outputId": "c4fb0e1c-86eb-4faf-c05d-5dfa8caa3121"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "\n",
            "Multi-Agent RAG System\n",
            "1. Ingest documents for an agent\n",
            "2. Ask a cross-domain question\n",
            "3. View communication history\n",
            "4. Exit\n",
            "Enter your choice (1-4): 1\n",
            "Enter agent name (TechAgent, ScientificAgent, BusinessAgent, HistoryAgent): /content/1.pdf\n",
            "Enter the directory path containing documents: /content/1.pdf\n",
            "Agent /content/1.pdf not found.\n",
            "\n",
            "Multi-Agent RAG System\n",
            "1. Ingest documents for an agent\n",
            "2. Ask a cross-domain question\n",
            "3. View communication history\n",
            "4. Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f8a50d517cbd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-f8a50d517cbd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1-4): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    PDFMinerLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "class RAGConfig:\n",
        "    def __init__(self):\n",
        "        self.embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "        self.llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        self.vector_db_path = \"vector_db\"\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.7\n",
        "        self.max_iterations = 3\n",
        "        self.thinking_steps = True\n",
        "        self.learning_rate = 0.1  # Dynamic learning rate\n",
        "\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    \"\"\"\n",
        "    Advanced knowledge representation system that tracks\n",
        "    relationships between concepts, confidence levels,\n",
        "    and sources of information.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.nodes = {}  # Concept nodes\n",
        "        self.edges = {}  # Relationships between concepts\n",
        "\n",
        "    def add_concept(self, concept: str, initial_confidence: float = 0.5):\n",
        "        \"\"\"Add a new concept to the knowledge graph.\"\"\"\n",
        "        if concept not in self.nodes:\n",
        "            self.nodes[concept] = {\n",
        "                'confidence': initial_confidence,\n",
        "                'sources': set(),\n",
        "                'related_concepts': set()\n",
        "            }\n",
        "\n",
        "    def add_relationship(self, concept1: str, concept2: str, relationship_type: str):\n",
        "        \"\"\"\n",
        "        Add a relationship between two concepts.\n",
        "        Relationship types could be: 'is_a', 'part_of', 'related_to', etc.\n",
        "        \"\"\"\n",
        "        if concept1 not in self.nodes:\n",
        "            self.add_concept(concept1)\n",
        "        if concept2 not in self.nodes:\n",
        "            self.add_concept(concept2)\n",
        "\n",
        "        if concept1 not in self.edges:\n",
        "            self.edges[concept1] = {}\n",
        "\n",
        "        self.edges[concept1][concept2] = {\n",
        "            'type': relationship_type,\n",
        "            'confidence': 0.5\n",
        "        }\n",
        "\n",
        "        # Bidirectional tracking of related concepts\n",
        "        self.nodes[concept1]['related_concepts'].add(concept2)\n",
        "        self.nodes[concept2]['related_concepts'].add(concept1)\n",
        "\n",
        "    def update_confidence(self, concept: str, new_info_confidence: float, source: str):\n",
        "        \"\"\"\n",
        "        Update the confidence of a concept based on new information.\n",
        "        Uses a weighted learning approach.\n",
        "        \"\"\"\n",
        "        if concept not in self.nodes:\n",
        "            self.add_concept(concept)\n",
        "\n",
        "        node = self.nodes[concept]\n",
        "        node['sources'].add(source)\n",
        "\n",
        "        # Weighted average of existing confidence and new information\n",
        "        node['confidence'] = (\n",
        "            (1 - self.config.learning_rate) * node['confidence'] +\n",
        "            self.config.learning_rate * new_info_confidence\n",
        "        )\n",
        "\n",
        "    def get_concept_confidence(self, concept: str) -> float:\n",
        "        \"\"\"Retrieve the confidence level of a concept.\"\"\"\n",
        "        return self.nodes.get(concept, {}).get('confidence', 0)\n",
        "\n",
        "    def find_related_concepts(self, concept: str, max_depth: int = 2) -> List[str]:\n",
        "        \"\"\"\n",
        "        Find concepts related to the given concept within a specified depth.\n",
        "        Uses a breadth-first search approach.\n",
        "        \"\"\"\n",
        "        related = set()\n",
        "        visited = set()\n",
        "        queue = [(concept, 0)]\n",
        "\n",
        "        while queue:\n",
        "            current_concept, depth = queue.pop(0)\n",
        "\n",
        "            if depth > max_depth:\n",
        "                break\n",
        "\n",
        "            if current_concept in visited:\n",
        "                continue\n",
        "\n",
        "            visited.add(current_concept)\n",
        "\n",
        "            if current_concept != concept:\n",
        "                related.add(current_concept)\n",
        "\n",
        "            # Add neighboring concepts to the queue\n",
        "            if current_concept in self.nodes:\n",
        "                for neighbor in self.nodes[current_concept]['related_concepts']:\n",
        "                    if neighbor not in visited and depth < max_depth:\n",
        "                        queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return list(related)\n",
        "\n",
        "\n",
        "class AdvancedSpecializedAgent:\n",
        "    def __init__(self, name: str, domain: str, config: RAGConfig):\n",
        "        \"\"\"\n",
        "        Enhanced agent with dynamic learning capabilities.\n",
        "        \"\"\"\n",
        "        self.id = str(uuid.uuid4())  # Unique identifier for the agent\n",
        "        self.name = name\n",
        "        self.domain = domain\n",
        "        self.config = config\n",
        "\n",
        "        # Core components\n",
        "        self.embedding_model = SentenceTransformerEmbedding(config)\n",
        "        self.vector_store = VectorStore(config, self.embedding_model)\n",
        "        self.llm = LocalLLM(config)\n",
        "\n",
        "        # Advanced reasoning systems\n",
        "        self.knowledge_graph = KnowledgeGraph()\n",
        "        self.reasoning_cache = {}  # Cache for complex reasoning results\n",
        "\n",
        "        # Learning and meta-cognitive tracking\n",
        "        self.learning_history = []\n",
        "        self.confidence_threshold = 0.7  # Confidence level for definitive answers\n",
        "\n",
        "        # Inter-agent communication protocol\n",
        "        self.communication_protocol = None\n",
        "\n",
        "    def learn_from_interaction(self, query: str, response: str, source_agent: str):\n",
        "        \"\"\"\n",
        "        Advanced learning mechanism that extracts and integrates knowledge.\n",
        "\n",
        "        This method:\n",
        "        1. Extracts key concepts from the interaction\n",
        "        2. Updates the knowledge graph\n",
        "        3. Tracks learning history\n",
        "        \"\"\"\n",
        "        # Extract key concepts using LLM\n",
        "        concept_extraction_prompt = f\"\"\"\n",
        "        From the following query and response, extract:\n",
        "        1. Key concepts\n",
        "        2. Relationships between concepts\n",
        "        3. Confidence level of information\n",
        "\n",
        "        Query: {query}\n",
        "        Response: {response}\n",
        "\n",
        "        Return JSON format:\n",
        "        {{\n",
        "            \"concepts\": [\n",
        "                {{\"name\": \"concept_name\", \"confidence\": 0.8}}\n",
        "            ],\n",
        "            \"relationships\": [\n",
        "                {{\"concept1\": \"A\", \"concept2\": \"B\", \"type\": \"is_a\"}}\n",
        "            ]\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            concept_json = json.loads(self.llm.generate(concept_extraction_prompt))\n",
        "\n",
        "            # Update knowledge graph\n",
        "            for concept in concept_json.get('concepts', []):\n",
        "                self.knowledge_graph.update_confidence(\n",
        "                    concept['name'],\n",
        "                    concept['confidence'],\n",
        "                    source_agent\n",
        "                )\n",
        "\n",
        "            # Add relationships\n",
        "            for rel in concept_json.get('relationships', []):\n",
        "                self.knowledge_graph.add_relationship(\n",
        "                    rel['concept1'],\n",
        "                    rel['concept2'],\n",
        "                    rel['type']\n",
        "                )\n",
        "\n",
        "            # Track learning event\n",
        "            self.learning_history.append({\n",
        "                'timestamp': time.time(),\n",
        "                'query': query,\n",
        "                'source_agent': source_agent,\n",
        "                'concepts_learned': [c['name'] for c in concept_json.get('concepts', [])]\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Learning extraction error: {e}\")\n",
        "\n",
        "    def generate_complex_reasoning(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Multi-step reasoning process that leverages the knowledge graph.\n",
        "\n",
        "        Steps:\n",
        "        1. Identify core concepts in the query\n",
        "        2. Explore related concepts in the knowledge graph\n",
        "        3. Generate a comprehensive reasoning path\n",
        "        \"\"\"\n",
        "        # Concept identification\n",
        "        concept_prompt = f\"\"\"\n",
        "        Identify the core concepts in this query that require reasoning:\n",
        "        Query: {query}\n",
        "\n",
        "        Return a JSON list of key concepts.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            core_concepts = json.loads(self.llm.generate(concept_prompt))\n",
        "\n",
        "            # Explore related concepts\n",
        "            related_concepts = []\n",
        "            for concept in core_concepts:\n",
        "                related_concepts.extend(\n",
        "                    self.knowledge_graph.find_related_concepts(concept)\n",
        "                )\n",
        "\n",
        "            # Comprehensive reasoning\n",
        "            reasoning_prompt = f\"\"\"\n",
        "            Reasoning Task:\n",
        "            Query: {query}\n",
        "\n",
        "            Core Concepts: {core_concepts}\n",
        "            Related Concepts: {related_concepts}\n",
        "\n",
        "            Provide a step-by-step reasoning process that:\n",
        "            1. Addresses the core query\n",
        "            2. Leverages relationships between concepts\n",
        "            3. Provides a nuanced, multi-perspective analysis\n",
        "            \"\"\"\n",
        "\n",
        "            reasoning_response = self.llm.generate(reasoning_prompt)\n",
        "\n",
        "            # Cache the reasoning for potential future use\n",
        "            cache_key = hash(query)\n",
        "            self.reasoning_cache[cache_key] = {\n",
        "                'response': reasoning_response,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            return reasoning_response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Complex reasoning error: {e}\")\n",
        "            return \"Unable to generate complex reasoning.\"\n",
        "\n",
        "    def query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Enhanced query method with multi-stage reasoning and learning.\n",
        "        \"\"\"\n",
        "        # Attempt to use cached reasoning first\n",
        "        cache_key = hash(query)\n",
        "        if cache_key in self.reasoning_cache:\n",
        "            cached_result = self.reasoning_cache[cache_key]\n",
        "            if time.time() - cached_result['timestamp'] < 3600:  # 1-hour cache\n",
        "                return {\n",
        "                    'response': cached_result['response'],\n",
        "                    'source': 'reasoning_cache',\n",
        "                    'confidence': 0.9\n",
        "                }\n",
        "\n",
        "        # Vector store retrieval\n",
        "        retrieved_docs = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # Complex reasoning generation\n",
        "        reasoning_response = self.generate_complex_reasoning(query)\n",
        "\n",
        "        # Prepare final response\n",
        "        final_response = {\n",
        "            'response': reasoning_response,\n",
        "            'retrieved_docs': [doc.page_content for doc in retrieved_docs],\n",
        "            'source': self.name,\n",
        "            'confidence': self.knowledge_graph.get_concept_confidence(query)\n",
        "        }\n",
        "\n",
        "        return final_response\n",
        "\n",
        "\n",
        "class AdvancedMultiAgentRAG:\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        \"\"\"\n",
        "        Advanced multi-agent system with dynamic learning and reasoning.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.agents = [\n",
        "            AdvancedSpecializedAgent(\"SyntheticIntelligenceAgent\", \"AI and cognitive systems\", config),\n",
        "            AdvancedSpecializedAgent(\"GlobalSystemsAgent\", \"complex interdisciplinary systems\", config),\n",
        "            AdvancedSpecializedAgent(\"EmergentTechnologiesAgent\", \"cutting-edge technological innovations\", config)\n",
        "        ]\n",
        "\n",
        "        # Collaborative learning environment\n",
        "        self.collaborative_knowledge_graph = KnowledgeGraph()\n",
        "\n",
        "    def cross_agent_learning(self, query: str, responses: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Facilitate learning across agents by sharing knowledge.\n",
        "        \"\"\"\n",
        "        for agent_name, response in responses.items():\n",
        "            for other_agent in self.agents:\n",
        "                if other_agent.name != agent_name:\n",
        "                    other_agent.learn_from_interaction(\n",
        "                        query,\n",
        "                        response['response'],\n",
        "                        agent_name\n",
        "                    )\n",
        "\n",
        "    def query(self, query: str) -> Dict[str, Dict]:\n",
        "        \"\"\"\n",
        "        Advanced query method with collaborative reasoning.\n",
        "        \"\"\"\n",
        "        responses = {}\n",
        "\n",
        "        # Parallel query processing\n",
        "        for agent in self.agents:\n",
        "            responses[agent.name] = agent.query(query)\n",
        "\n",
        "        # Cross-agent learning\n",
        "        self.cross_agent_learning(query, responses)\n",
        "\n",
        "        return responses\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = RAGConfig()\n",
        "    multi_agent_rag = AdvancedMultiAgentRAG(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nAdvanced Multi-Agent RAG System\")\n",
        "        print(\"1. Ask a complex query\")\n",
        "        print(\"2. View agent learning histories\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            query = input(\"Enter a complex, multi-domain question: \")\n",
        "            responses = multi_agent_rag.query(query)\n",
        "\n",
        "            print(\"\\nResponses from different agents:\")\n",
        "            for agent_name, response in responses.items():\n",
        "                print(f\"\\n{agent_name}:\")\n",
        "                print(f\"Response: {response['response']}\")\n",
        "                print(f\"Confidence: {response['confidence']}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            for agent in multi_agent_rag.agents:\n",
        "                print(f\"\\n{agent.name} Learning History:\")\n",
        "                for event in agent.learning_history:\n",
        "                    print(f\"Timestamp: {time.ctime(event['timestamp'])}\")\n",
        "                    print(f\"Query: {event['query']}\")\n",
        "                    print(f\"Source Agent: {event['source_agent']}\")\n",
        "                    print(f\"Concepts Learned: {event['concepts_learned']}\\n\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Thank you for using Advanced Multi-Agent RAG. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "PijH6A4GUeII",
        "outputId": "55ad5e9e-ced1-4c32-d83b-eed9f1599eec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "Loading embedding model sentence-transformers/all-mpnet-base-v2...\n",
            "Embedding model loaded successfully\n",
            "Loading LLM model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "LLM model loaded successfully on cpu\n",
            "\n",
            "Advanced Multi-Agent RAG System\n",
            "1. Ask a complex query\n",
            "2. View agent learning histories\n",
            "3. Exit\n",
            "Enter your choice (1-3): 1\n",
            "Enter a complex, multi-domain question: e\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Vector store not initialized. Please create or load a vector store first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6252c957900d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-6252c957900d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a complex, multi-domain question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_agent_rag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nResponses from different agents:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6252c957900d>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Parallel query processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# Cross-agent learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6252c957900d>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Vector store retrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mretrieved_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Complex reasoning generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-04e6043a5325>\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;34m\"\"\"Search for similar documents to the query.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vector store not initialized. Please create or load a vector store first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Use the standard similarity_search method instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Vector store not initialized. Please create or load a vector store first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMpA43UK1lhW",
        "outputId": "ab6777c6-84a4-41d5-e0cd-aec89d579bf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 orjson-3.10.16 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bH01VxxbpfzW",
        "outputId": "d0b577e0-487d-47ae-b3c1-4d590fd7d0e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.46.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog>=2.4.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=310d655a86791e87da0fcbeb56cb4c240880dd3f64cc55928aff593776b61fcb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, uvloop, python-dotenv, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.9 flatbuffers-25.2.10 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "8f455f8b15a549da87d8f2f0c8ddc689"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import asyncio\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.api.types import Embedding, Documents, IDs\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import (\n",
        "    DirectoryLoader,\n",
        "    TextLoader,\n",
        "    PDFLoader,\n",
        "    CSVLoader,\n",
        "    UnstructuredMarkdownLoader\n",
        ")\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableConfig\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class ChromaDBManager:\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
        "        \"\"\"\n",
        "        Initialize ChromaDB client with custom configuration\n",
        "        \"\"\"\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(persist_directory, exist_ok=True)\n",
        "\n",
        "        # ChromaDB client configuration\n",
        "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
        "\n",
        "        # Collection cache\n",
        "        self._collections = {}\n",
        "\n",
        "    def create_collection(\n",
        "        self,\n",
        "        name: str,\n",
        "        embedding_function=None,\n",
        "        metadata: Optional[Dict] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create or get a ChromaDB collection\n",
        "\n",
        "        :param name: Unique name for the collection\n",
        "        :param embedding_function: Custom embedding function\n",
        "        :param metadata: Optional metadata for the collection\n",
        "        :return: ChromaDB collection\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Check if collection already exists\n",
        "            if name in self._collections:\n",
        "                return self._collections[name]\n",
        "\n",
        "            # Create or get collection\n",
        "            collection = self.client.create_collection(\n",
        "                name=name,\n",
        "                embedding_function=embedding_function,\n",
        "                metadata=metadata or {}\n",
        "            )\n",
        "\n",
        "            # Cache the collection\n",
        "            self._collections[name] = collection\n",
        "            return collection\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating collection {name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def add_documents(\n",
        "        self,\n",
        "        collection_name: str,\n",
        "        documents: Documents,\n",
        "        embeddings: List[Embedding],\n",
        "        ids: IDs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Add documents to a ChromaDB collection\n",
        "\n",
        "        :param collection_name: Name of the collection\n",
        "        :param documents: List of document texts\n",
        "        :param embeddings: List of embeddings\n",
        "        :param ids: List of unique document IDs\n",
        "        \"\"\"\n",
        "        collection = self._collections.get(collection_name)\n",
        "        if not collection:\n",
        "            raise ValueError(f\"Collection {collection_name} not found\")\n",
        "\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "    def query(\n",
        "        self,\n",
        "        collection_name: str,\n",
        "        query_embeddings: List[Embedding],\n",
        "        n_results: int = 5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Query a ChromaDB collection\n",
        "\n",
        "        :param collection_name: Name of the collection\n",
        "        :param query_embeddings: Embedding of the query\n",
        "        :param n_results: Number of results to return\n",
        "        :return: Query results\n",
        "        \"\"\"\n",
        "        collection = self._collections.get(collection_name)\n",
        "        if not collection:\n",
        "            raise ValueError(f\"Collection {collection_name} not found\")\n",
        "\n",
        "        return collection.query(\n",
        "            query_embeddings=query_embeddings,\n",
        "            n_results=n_results\n",
        "        )\n",
        "\n",
        "class MultiAgentRAGConfig:\n",
        "    def __init__(self):\n",
        "        # Embedding and model configurations\n",
        "        self.embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "        self.llm_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "        # Directory configurations\n",
        "        self.document_dir = \"./documents\"\n",
        "        self.chroma_dir = \"./chroma_db\"\n",
        "\n",
        "        # Processing parameters\n",
        "        self.chunk_size = 1000\n",
        "        self.chunk_overlap = 200\n",
        "        self.top_k_results = 5\n",
        "\n",
        "class SpecializedAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        domain: str,\n",
        "        config: MultiAgentRAGConfig,\n",
        "        chroma_manager: ChromaDBManager\n",
        "    ):\n",
        "        self.id = str(uuid.uuid4())\n",
        "        self.name = name\n",
        "        self.domain = domain\n",
        "        self.config = config\n",
        "        self.chroma_manager = chroma_manager\n",
        "\n",
        "        # Embedding setup\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=config.embedding_model\n",
        "        )\n",
        "\n",
        "        # Language model\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=config.llm_model,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        # Create a collection for this agent\n",
        "        self.collection = self.chroma_manager.create_collection(\n",
        "            name=f\"{name}_collection\",\n",
        "            embedding_function=self.embeddings.embed_documents\n",
        "        )\n",
        "\n",
        "    def ingest_documents(self, document_path: str):\n",
        "        \"\"\"\n",
        "        Ingest and process documents for the agent's domain\n",
        "        \"\"\"\n",
        "        # Document loaders for different file types\n",
        "        loader_mapping = {\n",
        "            \"*.txt\": TextLoader,\n",
        "            \"*.pdf\": PDFLoader,\n",
        "            \"*.csv\": CSVLoader,\n",
        "            \"*.md\": UnstructuredMarkdownLoader\n",
        "        }\n",
        "\n",
        "        # Text splitter\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.chunk_size,\n",
        "            chunk_overlap=self.config.chunk_overlap\n",
        "        )\n",
        "\n",
        "        # Collect and process documents\n",
        "        all_documents = []\n",
        "        for pattern, loader_cls in loader_mapping.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(\n",
        "                    document_path,\n",
        "                    glob=pattern,\n",
        "                    loader_cls=loader_cls\n",
        "                )\n",
        "                docs = loader.load()\n",
        "                all_documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {pattern} documents: {e}\")\n",
        "\n",
        "        # Split documents\n",
        "        splits = text_splitter.split_documents(all_documents)\n",
        "\n",
        "        # Prepare for ChromaDB ingestion\n",
        "        documents = [doc.page_content for doc in splits]\n",
        "        embeddings = self.embeddings.embed_documents(documents)\n",
        "        ids = [str(uuid.uuid4()) for _ in documents]\n",
        "\n",
        "        # Add to ChromaDB collection\n",
        "        self.chroma_manager.add_documents(\n",
        "            collection_name=f\"{self.name}_collection\",\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        return len(documents)\n",
        "\n",
        "    async def query(self, query: str):\n",
        "        \"\"\"\n",
        "        Query the agent's knowledge base\n",
        "        \"\"\"\n",
        "        # Embed the query\n",
        "        query_embedding = self.embeddings.embed_query(query)\n",
        "\n",
        "        # Perform ChromaDB query\n",
        "        results = self.chroma_manager.query(\n",
        "            collection_name=f\"{self.name}_collection\",\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=self.config.top_k_results\n",
        "        )\n",
        "\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join(results['documents'][0])\n",
        "\n",
        "        # Generate response using LLM\n",
        "        response_prompt = PromptTemplate.from_template(\n",
        "            \"\"\"You are a {domain} expert.\n",
        "            Context: {context}\n",
        "\n",
        "            Question: {query}\n",
        "\n",
        "            Provide a comprehensive and insightful answer based on the context.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Create response chain\n",
        "        response_chain = (\n",
        "            response_prompt\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        response = await response_chain.ainvoke({\n",
        "            \"domain\": self.domain,\n",
        "            \"context\": context,\n",
        "            \"query\": query\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"agent\": self.name,\n",
        "            \"response\": response,\n",
        "            \"retrieved_docs\": results['documents'][0]\n",
        "        }\n",
        "\n",
        "class MultiAgentRAGSystem:\n",
        "    def __init__(self, config: MultiAgentRAGConfig):\n",
        "        # ChromaDB Manager\n",
        "        self.chroma_manager = ChromaDBManager(\n",
        "            persist_directory=config.chroma_dir\n",
        "        )\n",
        "\n",
        "        # Create specialized agents\n",
        "        self.agents = [\n",
        "            SpecializedAgent(\n",
        "                \"TechInnovationAgent\",\n",
        "                \"technology and innovation\",\n",
        "                config,\n",
        "                self.chroma_manager\n",
        "            ),\n",
        "            SpecializedAgent(\n",
        "                \"ScientificResearchAgent\",\n",
        "                \"scientific research\",\n",
        "                config,\n",
        "                self.chroma_manager\n",
        "            ),\n",
        "            SpecializedAgent(\n",
        "                \"GlobalSystemsAgent\",\n",
        "                \"complex global systems\",\n",
        "                config,\n",
        "                self.chroma_manager\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def ingest_documents(self):\n",
        "        \"\"\"\n",
        "        Ingest documents for all agents\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for agent in self.agents:\n",
        "            doc_count = agent.ingest_documents(config.document_dir)\n",
        "            results[agent.name] = doc_count\n",
        "        return results\n",
        "\n",
        "    async def cross_domain_query(self, query: str):\n",
        "        \"\"\"\n",
        "        Execute query across multiple agents asynchronously\n",
        "        \"\"\"\n",
        "        # Parallel query execution\n",
        "        agent_responses = await asyncio.gather(\n",
        "            *[agent.query(query) for agent in self.agents]\n",
        "        )\n",
        "\n",
        "        # Synthesize responses using the first agent's LLM\n",
        "        synthesizer = self.agents[0].llm\n",
        "        synthesis_prompt = PromptTemplate.from_template(\n",
        "            \"\"\"Synthesize responses from multiple domain experts:\n",
        "\n",
        "            Query: {query}\n",
        "\n",
        "            Agent Responses:\n",
        "            {responses}\n",
        "\n",
        "            Provide a comprehensive, multi-perspective answer\n",
        "            that integrates insights from different domains.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Synthesize response\n",
        "        synthesis_chain = (\n",
        "            synthesis_prompt\n",
        "            | synthesizer\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        synthesized_response = await synthesis_chain.ainvoke({\n",
        "            \"query\": query,\n",
        "            \"responses\": str(agent_responses)\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"agent_responses\": agent_responses,\n",
        "            \"synthesized_response\": synthesized_response\n",
        "        }\n",
        "\n",
        "async def main():\n",
        "    # Configuration\n",
        "    config = MultiAgentRAGConfig()\n",
        "\n",
        "    # Initialize RAG System\n",
        "    rag_system = MultiAgentRAGSystem(config)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChroma Multi-Agent RAG System\")\n",
        "        print(\"1. Ingest Documents\")\n",
        "        print(\"2. Ask a Cross-Domain Question\")\n",
        "        print(\"3. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-3): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Ingest documents\n",
        "            ingest_results = rag_system.ingest_documents()\n",
        "            print(\"Document Ingestion Results:\")\n",
        "            for agent, count in ingest_results.items():\n",
        "                print(f\"{agent}: {count} documents processed\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            # Cross-domain query\n",
        "            query = input(\"Enter your complex question: \")\n",
        "\n",
        "            result = await rag_system.cross_domain_query(query)\n",
        "\n",
        "            print(\"\\nAgent Responses:\")\n",
        "            for response in result['agent_responses']:\n",
        "                print(f\"{response['agent']} says:\")\n",
        "                print(response['response'])\n",
        "                print(\"Retrieved Documents:\")\n",
        "                for doc in response['retrieved_docs']:\n",
        "                    print(f\"- {doc[:200]}...\")\n",
        "                print(\"\\n\")\n",
        "\n",
        "            print(\"\\nSynthesized Response:\")\n",
        "            print(result['synthesized_response'])\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "DhF8HO1tpcX2",
        "outputId": "94bb2b96-ebcf-4c05-f158-dbcd545f06f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'PDFLoader' from 'langchain.document_loaders' (/usr/local/lib/python3.11/dist-packages/langchain/document_loaders/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e4b65b221876>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from langchain.document_loaders import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mDirectoryLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mTextLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PDFLoader' from 'langchain.document_loaders' (/usr/local/lib/python3.11/dist-packages/langchain/document_loaders/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import torch\n",
        "from threading import Thread\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Import the RAG system components\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Import your RAG system classes\n",
        "# Assuming the original code is in a file called rag_system.py\n",
        "\n",
        "\n",
        "# Global variables\n",
        "rag_system = None\n",
        "config = None\n",
        "is_initialized = False\n",
        "document_status = \"No documents ingested yet\"\n",
        "model_status = \"System not initialized\"\n",
        "\n",
        "\n",
        "def initialize_system():\n",
        "    \"\"\"Initialize the RAG system in the background\"\"\"\n",
        "    global rag_system, config, is_initialized, model_status\n",
        "\n",
        "    try:\n",
        "        # Update status\n",
        "        model_status = \"Initializing models... This may take a few minutes\"\n",
        "\n",
        "        # Initialize configuration\n",
        "        config = RAGConfig()\n",
        "\n",
        "        # Initialize the system\n",
        "        rag_system = AgenticRAG(config)\n",
        "\n",
        "        # Update status\n",
        "        model_status = f\"System initialized successfully. Using device: {config.device}\"\n",
        "        is_initialized = True\n",
        "\n",
        "        # Try to load existing vector store\n",
        "        if os.path.exists(config.vector_db_path):\n",
        "            rag_system.vector_store.load_vector_store()\n",
        "            global document_status\n",
        "            document_status = \"Previously ingested documents loaded\"\n",
        "\n",
        "    except Exception as e:\n",
        "        model_status = f\"Error initializing system: {str(e)}\"\n",
        "        print(f\"Initialization error: {e}\")\n",
        "\n",
        "\n",
        "def init_background():\n",
        "    \"\"\"Start initialization in background thread\"\"\"\n",
        "    thread = Thread(target=initialize_system)\n",
        "    thread.daemon = True\n",
        "    thread.start()\n",
        "    return \"Initializing system in background. Please wait...\"\n",
        "\n",
        "\n",
        "def ingest_documents(directory_path):\n",
        "    \"\"\"Ingest documents from the specified directory\"\"\"\n",
        "    global rag_system, document_status\n",
        "\n",
        "    if not is_initialized:\n",
        "        return \"System not initialized yet. Please wait.\"\n",
        "\n",
        "    if not os.path.exists(directory_path):\n",
        "        return f\"Directory not found: {directory_path}\"\n",
        "\n",
        "    try:\n",
        "        # Update status\n",
        "        document_status = f\"Ingesting documents from {directory_path}...\"\n",
        "\n",
        "        # Ingest documents\n",
        "        rag_system.ingest_documents(directory_path)\n",
        "\n",
        "        # Update status\n",
        "        document_status = f\"Successfully ingested documents from {directory_path}\"\n",
        "        return f\"Successfully ingested documents from {directory_path}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error ingesting documents: {str(e)}\"\n",
        "        document_status = error_msg\n",
        "        return error_msg\n",
        "\n",
        "\n",
        "def query_system(query, thinking_steps=True, max_iterations=3):\n",
        "    \"\"\"Query the RAG system with the given question\"\"\"\n",
        "    global rag_system, config, is_initialized, document_status\n",
        "\n",
        "    if not is_initialized:\n",
        "        return \"System not initialized yet. Please wait.\"\n",
        "\n",
        "    if not rag_system.vector_store.vector_store:\n",
        "        try:\n",
        "            if not rag_system.vector_store.load_vector_store():\n",
        "                return \"No documents have been ingested. Please ingest documents first.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error loading vector store: {str(e)}\"\n",
        "\n",
        "    try:\n",
        "        # Update configuration\n",
        "        config.thinking_steps = thinking_steps\n",
        "        config.max_iterations = max_iterations\n",
        "\n",
        "        # Process query\n",
        "        start_time = time.time()\n",
        "        response = rag_system.query(query)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Return formatted response\n",
        "        time_taken = end_time - start_time\n",
        "        return f\"Response (took {time_taken:.2f} seconds):\\n\\n{response}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing query: {str(e)}\"\n",
        "\n",
        "\n",
        "def get_system_info():\n",
        "    \"\"\"Get information about the system configuration\"\"\"\n",
        "    global config, is_initialized, document_status, model_status\n",
        "\n",
        "    if not is_initialized:\n",
        "        return model_status\n",
        "\n",
        "    info = {\n",
        "        \"Status\": model_status,\n",
        "        \"Device\": config.device,\n",
        "        \"Embedding Model\": config.embedding_model_name,\n",
        "        \"LLM Model\": config.llm_model_name,\n",
        "        \"Document Status\": document_status,\n",
        "        \"Vector DB Path\": config.vector_db_path,\n",
        "        \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    return \"\\n\".join([f\"{k}: {v}\" for k, v in info.items()])\n",
        "\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_ui():\n",
        "    with gr.Blocks(title=\"AgenticRAG System\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\"# AgenticRAG System with Local LLM\")\n",
        "        gr.Markdown(\"A RAG system with local models for document ingestion and querying.\")\n",
        "\n",
        "        with gr.Tab(\"System Status\"):\n",
        "            init_button = gr.Button(\"Initialize System\", variant=\"primary\")\n",
        "            status_output = gr.Textbox(label=\"System Status\", value=\"System not initialized\", lines=10)\n",
        "            refresh_button = gr.Button(\"Refresh Status\")\n",
        "\n",
        "            init_button.click(init_background, outputs=status_output)\n",
        "            refresh_button.click(get_system_info, outputs=status_output)\n",
        "\n",
        "        with gr.Tab(\"Ingest Documents\"):\n",
        "            gr.Markdown(\"### Ingest Documents\")\n",
        "            dir_input = gr.Textbox(label=\"Document Directory Path\", placeholder=\"/path/to/documents\")\n",
        "            ingest_button = gr.Button(\"Ingest Documents\", variant=\"primary\")\n",
        "            ingest_output = gr.Textbox(label=\"Ingestion Status\", lines=5)\n",
        "\n",
        "            ingest_button.click(ingest_documents, inputs=dir_input, outputs=ingest_output)\n",
        "\n",
        "        with gr.Tab(\"Ask Questions\"):\n",
        "            gr.Markdown(\"### Query the System\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    query_input = gr.Textbox(label=\"Your Question\", placeholder=\"What would you like to know?\", lines=3)\n",
        "                    with gr.Row():\n",
        "                        thinking_checkbox = gr.Checkbox(label=\"Enable Thinking Steps\", value=True)\n",
        "                        iterations_slider = gr.Slider(minimum=1, maximum=5, value=3, step=1, label=\"Max Iterations\")\n",
        "\n",
        "                    query_button = gr.Button(\"Submit Query\", variant=\"primary\")\n",
        "\n",
        "            response_output = gr.Textbox(label=\"Response\", lines=15)\n",
        "\n",
        "            query_button.click(\n",
        "                query_system,\n",
        "                inputs=[query_input, thinking_checkbox, iterations_slider],\n",
        "                outputs=response_output\n",
        "            )\n",
        "\n",
        "        # Auto-initialize on startup\n",
        "        app.load(fn=init_background, outputs=status_output)\n",
        "\n",
        "    return app\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    ui = create_ui()\n",
        "    ui.launch(share=True, server_name=\"0.0.0.0\", server_port=7861)"
      ],
      "metadata": {
        "id": "0F7MHEXqBXHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "df4e4327-d647-4d83-8230-6116c391fc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c08bd68e232ea1e97d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c08bd68e232ea1e97d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulX38wCH3cr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyMZOlAcAea8d37kCNSsR0Vy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "249ae3e360a74498994d7ffe93199893": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6d4fe9dda84c7fa1c29c4e40867b81",
            "placeholder": "​",
            "style": "IPY_MODEL_2a721b251eea48198822c49b07380dd5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2a721b251eea48198822c49b07380dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "322d0f8e6e564cb388b60ee45bcf03db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4985ca2324874245b42d1e26a313d052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c51202cfe1b44f597ab5d4452c916f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e9d99dccd554c6c8419d6b6e3acdd63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6d4fe9dda84c7fa1c29c4e40867b81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d815108048d842bf8f7c78134f388d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_249ae3e360a74498994d7ffe93199893",
              "IPY_MODEL_d95ec3aeb7bf4e5a8dcf3eb968d35fef",
              "IPY_MODEL_f5162a6267484bca9e633a3c349484e8"
            ],
            "layout": "IPY_MODEL_322d0f8e6e564cb388b60ee45bcf03db"
          }
        },
        "d95ec3aeb7bf4e5a8dcf3eb968d35fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9d99dccd554c6c8419d6b6e3acdd63",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4985ca2324874245b42d1e26a313d052",
            "value": 2
          }
        },
        "e193fef3bcdc435896d9b7287e4e6ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5162a6267484bca9e633a3c349484e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e193fef3bcdc435896d9b7287e4e6ab1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c51202cfe1b44f597ab5d4452c916f6",
            "value": " 2/2 [00:25&lt;00:00, 11.37s/it]"
          }
        },
        "23bccefbbb904d4b9a194a38f9c47bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b08cfb48f1f4694b46fc15cbd7f3685",
              "IPY_MODEL_be8f50dd7e704a0c9412c125024acd3f",
              "IPY_MODEL_86ff4c24094c4fcab6d868ef13914bc0"
            ],
            "layout": "IPY_MODEL_d8573df9ea154f28a2293b0c98295360"
          }
        },
        "8b08cfb48f1f4694b46fc15cbd7f3685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_781c3342a7184f3db2742735a40c56d4",
            "placeholder": "​",
            "style": "IPY_MODEL_849ef973f82445039fc1ab822a20d0b2",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "be8f50dd7e704a0c9412c125024acd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9974e739fe4482888a27267fd39869",
            "max": 793,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b927ed0f4d474e48bb1a76a97890d30c",
            "value": 793
          }
        },
        "86ff4c24094c4fcab6d868ef13914bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b89330dea1f4b3cb6231a51f6d389f3",
            "placeholder": "​",
            "style": "IPY_MODEL_715718e9bfaa45e5a0003ee8c0edf009",
            "value": " 793/793 [00:00&lt;00:00, 91.7kB/s]"
          }
        },
        "d8573df9ea154f28a2293b0c98295360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781c3342a7184f3db2742735a40c56d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849ef973f82445039fc1ab822a20d0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f9974e739fe4482888a27267fd39869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b927ed0f4d474e48bb1a76a97890d30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b89330dea1f4b3cb6231a51f6d389f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715718e9bfaa45e5a0003ee8c0edf009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aa1ab73daf14df3a9f2319dc3c00903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_089c898cdb61490fb9f94c96243808cf",
              "IPY_MODEL_e1b35ac7557c4d52a2d48a8bedba6eb9",
              "IPY_MODEL_a70a0fb9d9ec4acabba6292b56db3257"
            ],
            "layout": "IPY_MODEL_b677b73587cc47c983bfab42565cc961"
          }
        },
        "089c898cdb61490fb9f94c96243808cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c762d263b5d74699b8a8aa986220a846",
            "placeholder": "​",
            "style": "IPY_MODEL_de2d73886a0b40ff89bc3aa3536ec1bc",
            "value": "tokenizer.json: 100%"
          }
        },
        "e1b35ac7557c4d52a2d48a8bedba6eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bf33933c4844b0685902f778c9cb851",
            "max": 1367962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10a63cbadf724f10b7199f28cacc2fee",
            "value": 1367962
          }
        },
        "a70a0fb9d9ec4acabba6292b56db3257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fa4f21eef934c8ea0008844ec337726",
            "placeholder": "​",
            "style": "IPY_MODEL_de6b3b552b8e4a428e0353eee3a84fdd",
            "value": " 1.37M/1.37M [00:00&lt;00:00, 17.9MB/s]"
          }
        },
        "b677b73587cc47c983bfab42565cc961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c762d263b5d74699b8a8aa986220a846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2d73886a0b40ff89bc3aa3536ec1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bf33933c4844b0685902f778c9cb851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10a63cbadf724f10b7199f28cacc2fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fa4f21eef934c8ea0008844ec337726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6b3b552b8e4a428e0353eee3a84fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb61d2861c14d4abc2e8a79bffcf069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6995a35f3ef3473db8a8e43a4e0c35ef",
              "IPY_MODEL_c4098ab358384460b1022e609af11d10",
              "IPY_MODEL_6063b77726af4f4cbfdfb7f0f82439b0"
            ],
            "layout": "IPY_MODEL_ad9e4a822e784210b51de4e47af45ff1"
          }
        },
        "6995a35f3ef3473db8a8e43a4e0c35ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0031ab68e064419bf8773d2acc35be1",
            "placeholder": "​",
            "style": "IPY_MODEL_4801eb1ea7ae4b7f92376b849c47b825",
            "value": "config.json: 100%"
          }
        },
        "c4098ab358384460b1022e609af11d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b12394770ea44bd90bdda009c194f4f",
            "max": 632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49ae0e87ed82480585d9ff6265fee9c5",
            "value": 632
          }
        },
        "6063b77726af4f4cbfdfb7f0f82439b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afebecf506b94d929445fa7cc9ce7f6f",
            "placeholder": "​",
            "style": "IPY_MODEL_262a6f7bf8664ceabbffbbf43a7e6fc6",
            "value": " 632/632 [00:00&lt;00:00, 92.9kB/s]"
          }
        },
        "ad9e4a822e784210b51de4e47af45ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0031ab68e064419bf8773d2acc35be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4801eb1ea7ae4b7f92376b849c47b825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b12394770ea44bd90bdda009c194f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ae0e87ed82480585d9ff6265fee9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afebecf506b94d929445fa7cc9ce7f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262a6f7bf8664ceabbffbbf43a7e6fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2f4de0956b1496c88fc002617c3ad04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c906a5fd23bc492daf94360598b8df67",
              "IPY_MODEL_00d2f7dc5c5e48ae988615dfce6660f0",
              "IPY_MODEL_b0aeac7f134744478eeb1bdd3bc9d204"
            ],
            "layout": "IPY_MODEL_48d859d1d0de419a8db57357bc30751a"
          }
        },
        "c906a5fd23bc492daf94360598b8df67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6471bf6059b54433965077fef4c7122d",
            "placeholder": "​",
            "style": "IPY_MODEL_20f3be16a8a1403aa3535752f65aa6d2",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "00d2f7dc5c5e48ae988615dfce6660f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4833e625da3b432fa8c58f701c6fb79d",
            "max": 25125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a8c3ad80f954d0da18809d60bf0842d",
            "value": 25125
          }
        },
        "b0aeac7f134744478eeb1bdd3bc9d204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_781bfd1107354bb8acb507f5b05ed791",
            "placeholder": "​",
            "style": "IPY_MODEL_376b74a25e394b669ed4a5a862e31b94",
            "value": " 25.1k/25.1k [00:00&lt;00:00, 3.22MB/s]"
          }
        },
        "48d859d1d0de419a8db57357bc30751a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6471bf6059b54433965077fef4c7122d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f3be16a8a1403aa3535752f65aa6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4833e625da3b432fa8c58f701c6fb79d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a8c3ad80f954d0da18809d60bf0842d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "781bfd1107354bb8acb507f5b05ed791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376b74a25e394b669ed4a5a862e31b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72396654f2f14554836b378b5718b1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fd45d091b644a838e99b1f54cb7b2dd",
              "IPY_MODEL_7cc590b7765d4e21b56e7ef96b4dc7a4",
              "IPY_MODEL_45963d3c22ff4696ab344baf99c26f39"
            ],
            "layout": "IPY_MODEL_2dc35c24871b4d6f9fb8ab1d5968d46e"
          }
        },
        "0fd45d091b644a838e99b1f54cb7b2dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6718531a8a1a4e0894a12aee2d321b05",
            "placeholder": "​",
            "style": "IPY_MODEL_fe8991a0594746cd83ff681f13db9318",
            "value": "Downloading shards: 100%"
          }
        },
        "7cc590b7765d4e21b56e7ef96b4dc7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f714f06b74c4359a96c64856c5cacd6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbb2c608fd484dcc81555c1e27def387",
            "value": 2
          }
        },
        "45963d3c22ff4696ab344baf99c26f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd6589f04de84e51b382838ae34fdc2b",
            "placeholder": "​",
            "style": "IPY_MODEL_dc03f6a851ca424cad5b38c75f204a8a",
            "value": " 2/2 [01:33&lt;00:00, 44.22s/it]"
          }
        },
        "2dc35c24871b4d6f9fb8ab1d5968d46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6718531a8a1a4e0894a12aee2d321b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe8991a0594746cd83ff681f13db9318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f714f06b74c4359a96c64856c5cacd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbb2c608fd484dcc81555c1e27def387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd6589f04de84e51b382838ae34fdc2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc03f6a851ca424cad5b38c75f204a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28dd8ba74823470dbca2a8984cb60416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e57e0e9c6dc46f7920abf80b488b0fe",
              "IPY_MODEL_97ccfa9d1107401397592870090ac065",
              "IPY_MODEL_31685198b525456ea1885759492cb155"
            ],
            "layout": "IPY_MODEL_a95a5441f3c34917b60888beba143bfe"
          }
        },
        "3e57e0e9c6dc46f7920abf80b488b0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2add0ab6ed374b858f6f2511205aa74c",
            "placeholder": "​",
            "style": "IPY_MODEL_32df62147db0465fb0ee160650d32a1a",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "97ccfa9d1107401397592870090ac065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e322e6f42c904b86a39510fa48bbdccb",
            "max": 9978667672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8740c537426743a3ae136c7348496a5b",
            "value": 9978667672
          }
        },
        "31685198b525456ea1885759492cb155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1eb5e1262d84f2b831029abc38f2a2c",
            "placeholder": "​",
            "style": "IPY_MODEL_10ee5da44cbc400abaf63fbb8e88bd5b",
            "value": " 9.98G/9.98G [00:59&lt;00:00, 156MB/s]"
          }
        },
        "a95a5441f3c34917b60888beba143bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2add0ab6ed374b858f6f2511205aa74c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32df62147db0465fb0ee160650d32a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e322e6f42c904b86a39510fa48bbdccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8740c537426743a3ae136c7348496a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1eb5e1262d84f2b831029abc38f2a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ee5da44cbc400abaf63fbb8e88bd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c773318ac9914b40886b5946e9eb9863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13efe2d32b9445118a3ccd4346d6c585",
              "IPY_MODEL_0d57fb7c0531470eb9b2a3a291b9ebd9",
              "IPY_MODEL_9983dcb363f9414f824014ad8648768d"
            ],
            "layout": "IPY_MODEL_a8127533eeca44a99e300dad7d6f88e4"
          }
        },
        "13efe2d32b9445118a3ccd4346d6c585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e90216632f754eefb2f6d5a7fc63c316",
            "placeholder": "​",
            "style": "IPY_MODEL_a0bb6cab2b784cde91694f43178bbd1f",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "0d57fb7c0531470eb9b2a3a291b9ebd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb0051de4b44db2a46bb9a6d2b470a4",
            "max": 3502391696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b00caf26f66472bb3e15e160b9f1e6c",
            "value": 3502391696
          }
        },
        "9983dcb363f9414f824014ad8648768d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb1573215334885a9262ab960c19f23",
            "placeholder": "​",
            "style": "IPY_MODEL_f748c9d935b14ff2b47a95a83dbfccc0",
            "value": " 3.50G/3.50G [00:33&lt;00:00, 110MB/s]"
          }
        },
        "a8127533eeca44a99e300dad7d6f88e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e90216632f754eefb2f6d5a7fc63c316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bb6cab2b784cde91694f43178bbd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb0051de4b44db2a46bb9a6d2b470a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b00caf26f66472bb3e15e160b9f1e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccb1573215334885a9262ab960c19f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f748c9d935b14ff2b47a95a83dbfccc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df3c24cb37544ef389742edf307df828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_009dc548f27b478fb83cbbd74c4f5254",
              "IPY_MODEL_17f55230b71a4273acfd49224773442a",
              "IPY_MODEL_4b0e78aa810f4479a5da72f229231ead"
            ],
            "layout": "IPY_MODEL_4f638fb86d774ecaa3408c83eec1b543"
          }
        },
        "009dc548f27b478fb83cbbd74c4f5254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_119e27240c334322b0bd8b25594d4452",
            "placeholder": "​",
            "style": "IPY_MODEL_e9fef4bdad4a43a18fabff54a2c51c57",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "17f55230b71a4273acfd49224773442a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3713ef908e02410b850dd67848575076",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_534e73f8fd1944caa539c3caf3b8e2a2",
            "value": 2
          }
        },
        "4b0e78aa810f4479a5da72f229231ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27399dd450a84ef985de553fc85f9d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_91a8a324e3094afe8d5922f52d40dc8a",
            "value": " 2/2 [00:36&lt;00:00, 18.54s/it]"
          }
        },
        "4f638fb86d774ecaa3408c83eec1b543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "119e27240c334322b0bd8b25594d4452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9fef4bdad4a43a18fabff54a2c51c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3713ef908e02410b850dd67848575076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534e73f8fd1944caa539c3caf3b8e2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27399dd450a84ef985de553fc85f9d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a8a324e3094afe8d5922f52d40dc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e1d6682b7c64cf49b1867896a793dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9faffccf75f94d7bbaeaca82f26c6f1d",
              "IPY_MODEL_0a99a5afce9d409fad5e77a7010d697c",
              "IPY_MODEL_af3cd7b57f384959adc8cda78a0213e4"
            ],
            "layout": "IPY_MODEL_93b235f9be8e45dcb5c11f8d814fd5e4"
          }
        },
        "9faffccf75f94d7bbaeaca82f26c6f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19cc91a69e7452ea6c36631e5735487",
            "placeholder": "​",
            "style": "IPY_MODEL_2062979197694d4bba591641bcbc8c07",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0a99a5afce9d409fad5e77a7010d697c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4299877fa0345478680b34ac307a3f3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c669a243af274684948c0cb15ac10924",
            "value": 2
          }
        },
        "af3cd7b57f384959adc8cda78a0213e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e73f0126660404faddddcf997fc9ec2",
            "placeholder": "​",
            "style": "IPY_MODEL_cb64e53a7a094560a5f1c3145250b970",
            "value": " 2/2 [00:25&lt;00:00, 11.85s/it]"
          }
        },
        "93b235f9be8e45dcb5c11f8d814fd5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c19cc91a69e7452ea6c36631e5735487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2062979197694d4bba591641bcbc8c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4299877fa0345478680b34ac307a3f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c669a243af274684948c0cb15ac10924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e73f0126660404faddddcf997fc9ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb64e53a7a094560a5f1c3145250b970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}